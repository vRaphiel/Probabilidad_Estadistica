\documentclass[a4paper]{article}
\usepackage{amsmath}
\setlength{\parskip}{1em}
\input{Algo1Macros}

\begin{document}

\section{Ejemplos de estimacion puntual}

\subsection{Ejercicio 1 - Distribucion Normal}
Se quiere conocer el peso medio de los paquetes de arroz producido por una fábrica. Para ello se toman 30 cajas
de arroz al azar y se las pesa. Se obtiene

\begin{equation*}
    \begin{matrix}
        0.96 & 0.97 & 1.12 & 1.16 & 1.03 & 0.95 & 0.91 & 0.87 & 0.96 & 1.04 \\
        0.77 & 0.99 & 0.84 & 1.08 & 1.12 & 0.78 & 0.95 & 0.93 & 1.09 & 0.92 \\
        1.00 & 0.92 & 1.02 & 0.90 & 0.87 & 0.85 & 1.03 & 1.04 & 0.92 & 1.07
    \end{matrix}
\end{equation*}

Supongamos que el peso de un paquete elegido al azar es una variable aleatoria $X \sim \mathcal{N}(\mu,\sigma^2)$

Al elegir $n$ paquetes tenemos: Sea $X_{1},X_{2},\dots,X_{n}$ i.i.d. $X_{i} \sim \mathcal{N}(\mu,\,\sigma^{2})$
Estimador de los momentos de $\mu$ de orden 1:
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\mu}}(X) = \widehat{\mu}
\end{equation*}
Con estos datos la estimacion que se obtiene es $\widehat{\mu}_{obs} = 0.97$

\subsection{Ejercicio 2 - Distribucion Exponencial}
Una fabrica de lamparas sabe que el tiempo de vida, en dias, de las lamparas que fabrica, sigue una distribucion $Exp(\theta)$.
Obtener una formula para estimar $\theta$ a partir de una muestra aleatoria $X_{1}\dots X_{n}$

Antes de probar las lamparas no sabemos cuanto durara cada una. Asi la duracion de la primera puede ser considerada
una v.a $X_{1}$. la segunda una v.a $X_{2}$, etc. 

\begin{equation*}
    X_{1}, X_{2}, \dots, X_{n} \sim Exp(\theta)
\end{equation*}

Para hallar el estimador de momentos de $\theta$, hay que despejar $\widehat{\theta}$ de
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\theta}}(X_{1})
\end{equation*}
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = \frac{1}{\mathcal{\theta}} \Rightarrow \mathcal{\theta} = \frac{1}{\widehat{X}}
\end{equation*}

\subsection{Ejercicio 3 - Distribucion uniforme}
\begin{equation*}
    X_{1}, X_{2}, \dots , X_{n} \sim \mathcal{U}{0, \theta}
\end{equation*}
Hay que despejar $\theta$ de
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\theta}}(X_{1})
\end{equation*}
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = \frac{\widehat{\theta}}{2} \Rightarrow \mathcal{\theta} = 2\frac{1}{n}\sum_{i=1}^{n} X_{i}
\end{equation*}

\subsection{Ejercicio 4 - Estimacion de ambos parametros de la normal}
Supongamos que tenemos una muestra aleatoria $X_{1}, X_{2},\dots,X_{n} \sim \mathcal{N}(\mu, \sigma^2)$ \\
Se tiene que $E_{\mu, \sigma^2}(X) = \mu$ y $E_{\mu, \sigma^2}(X^2) = \mu^2 + \sigma^2$
Para encontrar el estimador de momentos de $\mu$ y $\sigma$ hay que resolver el sistema

\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\mu}, \widehat{\sigma}^2}(X_{1}) = \widehat{\mu}
\end{equation*}
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i}^2 = E_{\widehat{\mu}, \widehat{\sigma}^2}(X_{1}^2) = \widehat{\mu}^2 + \widehat{\sigma}^2
\end{equation*}

\begin{equation*}
    \begin{cases}
        \frac{1}{n} \sum_{i=1}^n X_{i} = \widehat{\mu}
        \\
        \frac{1}{n} \sum_{i=1}^n X_{i}^2 = \widehat{\mu}^2 + \widehat{\sigma}^2
        \end{cases}
\end{equation*}
\begin{equation*}
    \widehat{\mu} = \frac{1}{n} \sum_{i=1}^n X_{i}
\end{equation*}
\begin{equation*}
    \widehat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_{i}^2 - (\frac{1}{n} \sum_{i=1}^n X_{i})^2
\end{equation*}

\section{Ejemplos de estimacion puntual - Verosimilitud}
\subsection{Ejercicio 1 - $\mathcal{E}(\lambda):f(x,\lambda) = \lambda e^{-x\lambda}\mathcal{I}_{(0, \infty)}(x)$}
$X_{1},\dots,X_{n}$ v.a. i.i.d. $X_{i} \sim \mathcal{E}(\lambda), \lambda > 0$
\begin{equation*}
    L(\lambda; x) = \prod_{i=1}^n f(x_{i}, \lambda) = \prod_{i=1}^n \lambda e^{-x_{i}\lambda}\mathcal{I}_{(0,\infty)}(x_{i})
\end{equation*}
Si $x_{i}\geq 0 \forall i$
\begin{equation*}
    L(\lambda; x) = \lambda e^{-x_{i}\sum_{i=1}^n x_{i}}
\end{equation*}
Si consideramos $\log L$ resulta
\begin{equation*}
    l(\lambda;x) = n \log(\lambda)-\lambda \sum_{i=1}^n x_{i}
\end{equation*}
Derivando e igualando a 0 queda \\
$\frac{n}{\lambda} - \sum_{i=1}^n x_{i} = 0 \Rightarrow$ punto critico es $\frac{1}{\bar{x}_{n}}$, ver que maximiza \\
$\Rightarrow \widehat{\lambda} = \frac{1}{\bar{X}_{n}}$

\subsection{Ejercicio 2 - $X_{1},\dots,X_{n}$ v.a. i.i.d. $X_{i} \sim \mathcal{N} (\mu, 9)$, $f(x,\mu, 9) = \frac{1}{\sqrt{2\pi}}\frac{1}{3}e^{-\frac{1}{2}\frac{(x-\mu)^2}{9}}$ }
\begin{equation*}
    L(\mu, 9; x) = \prod_{i=1}^n f(x_{i}, \mu, 9) = \prod_{i=1}^n \frac{1}{\sqrt(2\pi)}\frac{1}{3}e^{-\frac{1}{2}\frac{(x_{i}-\mu)^2}{9}}
\end{equation*}
\begin{equation*}
    L(\mu, 9; x) = (\frac{1}{\sqrt(2\pi)})^n(\frac{1}{3})^n e^{-\frac{1}{2}\frac{\sum_{i=1}^n(x_{i}-\mu)^2}{9}}
\end{equation*}
Tomemos logatirmo
\begin{equation*}
    l(\mu, 9; x) = cte - \frac{1}{2}\frac{\sum_{i=1}^n(x_{i}-\mu)^2}{9}
\end{equation*}
Maximizar a $l(\mu, 9; x)$ como funcion de $\mu$ equivale a minimizar
\begin{equation*}
    h(\mu) = \sum_{i=1}^n (x_{i} - \mu)^2
\end{equation*}
Un par de clases aatras vimos que $h(\mu)$ se minimiza en $\bar{x}_{n}$
\begin{equation*}
    EMV de \ \mu : \widehat{\mu} = \bar{X}_{n}
\end{equation*}
\begin{equation*}
    L(\mu, 9; x) = \prod_{i=1}^n f(x_{i}, \mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt(2\pi\sigma^2)}e^{-\frac{(x_{i}-\mu)^2}{2\sigma^2}}
\end{equation*}
\begin{equation*}
    L(\mu, 9; x) = \prod_{i=1}^n f(x_{i}, \mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt(2\pi\sigma^2)}e^{-\sum_{i=1}^n \frac{(x_{i}-\mu)^2}{2\sigma^2}}
\end{equation*}
Tomando logatirmo y resolviendo las ecuaciones
\begin{equation*}
    \frac{\partial l (\mu, \sigma^2;x)}{\partial \mu} = 0 \ y \ \frac{\partial l (\mu, \sigma^2;x)}{\partial \sigma^2} = 0
\end{equation*}
se obtiene que los EMV de $\mu$ y $\sigma^2$ section
\begin{equation*}
    \widehat{\mu} = \bar{X}_{n} \ \ \ \ \widehat{\sigma}^2 = \frac{\sum_{i=1}^n ((X_{i} - \bar{X}_n))^2}{n}
\end{equation*}
Los estimadores de $\mu$ y $\sigma$ coinciden con los estimadores de momentos

\subsection{Ejercicio 2 - $\mathcal{U}(0,\theta):f(x,\theta) = \frac{1}{\theta}\mathcal{I}_{(0,\theta)}(x)$}
$X_{1},\dots,X_{n}$ v.a. i.i.d. $X_{i} \sim \mathcal{U}(0,\theta)$
\begin{equation*}
    f(x_{1},\dots,x_{n}, \theta) = \prod_{i=1}^n \frac{1}{\theta}I_{(0,\theta)}(x_{i}) = \frac{1}{\theta^n}\prod_{i=1}^n I_{(0,\theta)}(x_{i})
\end{equation*}
La indicadora vale 1 cuando todas las indicadoras valgan 1, de lo contrario, es 0
Por ende, la conjunta puede tomar 2 valores:
\begin{equation*}
    \begin{cases}
        \frac{1}{\theta^n} \ si \ 0<x_{i}<\theta \  \forall i
        \\
        0 \ en \ otro \ caso
        \end{cases}
\end{equation*}
\begin{equation*}
    \begin{cases}
        \frac{1}{\theta^n} \ si \ \theta > \max(x_{i})
        \\
        0 \ en \ otro \ caso
        \end{cases}
\end{equation*}

\pagebreak

\section{Propiedades de estimacion}
\subsection{Ejercicio 1 - Distribucion Uniforme}
\begin{itemize}
    \item $X_i \sim \mathcal{U}(0, \theta)$
    \item $\widehat{\theta} = 2\bar{X}_n$ (Estimador de momentos)
    \item $\widehat{\theta} = max{X_{1},\dots,X_{n}}$ (Estimador de maxima Verosimilitud)
    \item Calcule la esperanza y varianza de cada estimador
\end{itemize}

Esperanza del estimador de momentos
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$. El estimador de momentos de $\theta$ es $\widehat{\theta} = 2\bar{X}$

\begin{equation*}
    E_{\theta}(\widehat{\theta}) = 2E_{\theta}\bar{X} = 2\frac{\theta}{2} = \theta \ \ \forall \theta
\end{equation*}

Recordar

Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$
\begin{itemize}
    \item El EMV de $\theta = max_{1\leq i \leq n}(X_{i})$
    \item La fda es $\widehat{\theta}$ es
        \begin{equation*}
            F_{\widehat{\theta}}(u) = (F_{X_{i}}(u))^n =       
            \begin{cases}
                0 \ si \ u \leq \theta
                \\
                (\frac{u}{\theta})^n \ si \ 0<u<\theta
                \\
                1 \ si \ u \geq \theta
            \end{cases}
        \end{equation*}
    \item La densidad de $\widehat{\theta}$ es
        \begin{equation*}
            f_{\widehat{\theta}}(u) = n\frac{u}{\theta}^{n-1} \frac{1}{\theta} I_{(0,\theta)}(u)
        \end{equation*}
\end{itemize}

\begin{equation*}
    E_{\theta}(\widehat{\theta}) = \int_{0}^{\theta} un(\frac{u}{\theta})^{n-1} \frac{1}{\theta} du = 
    \\
    \frac{n}{\theta^n}\int_{0}^{\theta} u^n du = \frac{n}{\theta^n}\frac{u^{n+1}}{n+1}\Biggr|_{0}^{\theta} = \frac{n}{n+1}\theta
\end{equation*}

Varianza del estimador de momentos

Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$ y $\widehat{\theta}=2\bar{X}$ el estimador de momentos de $\theta$
\begin{equation*}
    V_{\theta}(\widehat{\theta}) = 4V_{\theta}(\bar{X}) = 4 \frac{\theta^2/12}{n}
\end{equation*}

Varianza del estimador de maxima Verosimilitud
Recordar la densidad del EMV
\begin{gather*}
    f_{\theta}(u) = n\frac{u}{\theta}^{n-1}\frac{1}{\theta}I_{(0,\theta)}(u)
    \\
    E_{\theta}(\widehat{\theta}^2) = \int_{0}^{\theta} u^2n \frac{u}{\theta}^{n-1} \frac{1}{\theta}du = \frac{n}{\theta^n}\int_{0}^{\theta}u^{n+1} du
    \\
    \frac{n}{\theta^n}\frac{u^{n+2}}{n+2}\Biggr|_{0}^{\theta} = \frac{n}{n+2}\theta^2
\end{gather*}
Entonces,
\begin{gather*}
    V_{\theta}(\widehat{\theta}) = \frac{n}{n+2}\theta^2 - (\frac{n}{n+1})^2\theta^2=(\frac{n}{n+2} - \frac{n^2}{(n+1)^2})\theta^2
    \\
    \frac{n}{(n+2)(n+1)^2}\theta^2
\end{gather*}

\subsection{Ejercicio 2 - Sesgo de los EMV de $\mu$ y $\sigma^2$ en el caso normal}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{N}(\mu, \sigma^2)$. Los EMV de $\mu$ y $\sigma^2$ section
\begin{equation*}
    \widehat{\mu} = \bar{X} \ \ \ \ \widehat{\sigma}^2 = \frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{n}
\end{equation*}

\begin{itemize}
    \item El estimador de $\mu$ es insesgado pues $E_{\mu, \sigma^2}(\widehat{\mu}) = \mu \forall\mu$,
    \item El estimador de $\sigma^2$ es sesgado pero es asintoticamente insesgado pues:
\end{itemize}
\begin{gather*}
    E_{\mu, \sigma^2}(\widehat{\sigma}^2) = E_{\mu, \sigma^2}(\frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{n})
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n (X_{i}^2 - 2X_{i}\bar{X} + \bar{X}^2))
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2 - 2\bar{X}\sum_{i=1}^n X_{i} + n\bar{X}^2)
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2 - 2n\bar{X}^2 + n\bar{X}^2)
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2 - n\bar{X}^2)
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2) - E_{\mu, \sigma^2}(\bar{X}^2)
    \\
    = \frac{n}{n}E_{\mu, \sigma^2}(X_{1}^2) - E_{\mu, \sigma^2}(\bar{X}^2)
    \\
    = [ V_{\mu, \sigma^2}(X_{1}) + (E_{\mu, \sigma^2}(X_{1}))^2] - [ V_{\mu, \sigma^2}(\bar{X}) + (E_{\mu, \sigma^2}(\bar{X_{1}}))^2]
    \\
    = \sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2
    \\
    = \frac{n-1}{n}\sigma^2
\end{gather*}

\subsection{Ejemplo - Consistencia de la media muestral}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion con $E(X_{i})=\mu$ y $V(X_{i}) = \sigma^2 < \infty$ 
Sea $\widehat{\mu} = \bar{X}$
\begin{itemize}
    \item $E(\bar{X}) = \mu$
    \item $V(\bar{X}) = \frac{\sigma^2}{n}$
\end{itemize}
Entonces $\widehat{\mu}$ es un estimador consistente de $\mu$

\subsection{Ejemplo - Consistencia de los estimadores de $\theta$ en la $\mathcal{U}[0,\theta]$}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$. Vimos que el EMV de $\theta, \widehat{\theta} = max_{1\leq i \leq n}(X_{i})$ y 
\begin{itemize}
    \item $E_{\theta}(\widehat{\theta}) = \frac{n}{n+1}\theta \Rightarrow \widehat{\theta}$ es asintóticamente insesgado
    \item $V_{\theta}(\widehat{\theta}) = \frac{n}{(n+2)(n+1)^2}\theta^2 \rightarrow_{n\rightarrow\infty}0$
\end{itemize}
Entonces el $\widehat{\theta}$ es consistente

\subsection{Ejemplo - Consistencia de la varianza muestral}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una con $E(X_{i}) = \mu$ y $V(X_{i}) = \sigma^2 < \infty$ entonces la varianza muestral $S^2$ es un estimador
consistente de la varianza poblacional
\begin{gather*}
    S^2 = \frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{n-1} = \frac{1}{n-1}(\sum_{i=1}^n X_{i}^2 - n\bar{X}^2)
    \\
    = \frac{n}{n-1}(\frac{\sum_{i=1}^n X_{i}^2}{n} - \bar{X}^2)
\end{gather*}

Por la Ley de los Grandes Numeros $\bar{X} \xrightarrow[]{p} \mu$, entonces por la propiedad 4,
\begin{equation*}
    \bar{X}^2 \xrightarrow[p]{} \mu^2
\end{equation*}

Por otra parte, aplicando nuevamente la Ley de los grandes Numeros
\begin{equation*}
    \frac{\sum_{i=1}^n X_{i}^2}{n} \xrightarrow[]{p} E_{\mu, \sigma^2}(X^2) = V_{\mu, \sigma^2}(X) + [E_{\mu, \sigma^2}(X)]^2 = \sigma^2 + \mu^2
\end{equation*}
Como ademas $\frac{n}{n-1} \rightarrow 1$, se obtiene
\begin{equation*}
    S_{x}^2 = \frac{n}{n-1}(\frac{\sum_{i=1}^n X_{i}^2}{n} - \bar{X}^2) \xrightarrow[]{p} \sigma^2 + \mu^2 -\mu^2 = \sigma^2
\end{equation*}
y por lo tanto la varianza muestral es un estimador consistente de $\sigma^2$

\pagebreak
\section{Intervalos de confianza}

\subsection{Ejemplo - Intervalo de confianza para el parametro de la exponencial}
\begin{itemize}
    \item Si $Z_1,\dots,Z_n$ i.i.d., $Z_i \sim \mathcal{N}(0,1)$, llamamos chi cuadrado con $n$ grados de libertad a la distribucion de
    \begin{equation*}
        Z_1^2 + Z_2^2 + \dots + Z_n^2
    \end{equation*}
    \item la suma de $n$ v.a. $\chi_1^2$ tiene distribucion $\chi_n^2$
    \item \textbf{Ejercicio 23 de la practica 3}. Sea $Z$ una v.a. con una distribucion normal standar. Pruebe que $Z^2 \sim \Gamma(1/2, 1/2)$
    (Esta variable aleatoria recibe el nombre de $\chi^2$ con un grado de libertad)
    \item Una suma de n v.a. $\chi_1^2$ es $\Gamma(n/2, 1/2)$. Por lo tanto,
    \begin{equation*}
        \Gamma(n/2, 1/2) = \chi_n^2
    \end{equation*}
\end{itemize}

Sea $X_1, X_2, \dots, X_n$ una m.a. de una distribucion $E(\lambda)$
Recordar:
\begin{itemize}
    \item La distribucion de la suma de exponenciales es Gamma, es decir
    \begin{equation*}
        \sum_{i=1}^n X_i \sim \Gamma(n, \lambda)
    \end{equation*}
    \item Una constante por una Gamma es Gamma:
    \item \begin{equation*}
        V \sim \Gamma(\alpha, \lambda) \ y \ a > 0 \Rightarrow aV \sim \Gamma(\alpha, \frac{\lambda}{\alpha})
    \end{equation*}
\end{itemize}
Por lo tanto
\begin{gather*}
    \sum_{i=1}^n X_i \sim \Gamma(n, \lambda)
    \\
    \lambda \sum_{i=1}^n X_i \sim \Gamma(n,1)
    \\
    2\lambda \sum_{i=1}^n X_i \sim \Gamma(n, \frac{1}{2}) = \Gamma(\frac{2n}{2}, \frac{1}{2}) = \chi_{2n}^2
\end{gather*}
Pivot para la exponencial:
\begin{equation*}
    H(X_1, X_2, \dots, X_n, \lambda) = 2\lambda \sum_{i=1}^n X_i
\end{equation*}
\begin{gather*}
    P\left(\chi_{2n,1-\alpha /2}^2 \leq 2\lambda \sum_{i=1}^n X_i \leq \chi_{2n,a/2}^2 \right) = 1-\alpha
    \\
    P\left(\frac{\chi_{2n,1-\alpha / 2}}{2\sum_{i=1}^n X_i} \leq \lambda \leq \frac{\chi_{2n,\alpha /2}}{2\sum_{i=1}^n X_i}\right) = 1-\alpha
\end{gather*}
Entonces el intervalo de confianza para $\lambda$
\begin{equation*}
    \left[\frac{\chi_{2n,1-\alpha / 2}}{2\sum_{i=1}^n X_i}, \frac{\chi_{2n,\alpha / 2}}{2\sum_{i=1}^n X_i} \right]
\end{equation*}

\subsection{Ejemplo - Invervalo de confianza para $\theta$ en la $\mathcal{U}[0, \theta]$}
Sea $X_1, X_2,\dots,X_n$ una m.a. de una distribucion $U(0, \theta)$
\begin{itemize}
    \item el EMV de $\theta$ es $\hat{\theta} = max(X_1,\dots,X_n)$ 
    \item la distribucion de $max(X_1,\dots,X_n)$ es $(F_{x_1}(u))^n$
    \item La fda de $\hat{\theta}$ esperanza
    \begin{equation*}
        F_{\hat{\theta}}(u) = (F_{x_1}(u))^n = 
        \begin{cases}
            0 \ si \ u\leq 0
            \\
            \left(\frac{u}{\theta}\right)^n \ si \ 0 < u < \theta
            \\
            1 \ si \ u\geq \theta
        \end{cases}
    \end{equation*}
    \item la densidad de $\hat{\theta}$ es
    \begin{equation*}
        f_{\hat{\theta}}(u) = n\left(\frac{u}{\theta}\right)^{n-1} \frac{1}{\theta} I_{(0,\theta)}(u)
    \end{equation*}
    Veamos que la distribucion de $\hat{\theta}/\theta$ no depende de $\theta$
\end{itemize}
Queremos demostrar que la distribucion de $\hat{\theta}/\theta$ no depende de $\theta$
\begin{equation*}
    F_{\hat{\theta} / \theta}(u) = P\left(\frac{\hat{\theta}}{\theta} \leq u \right) = P(\hat{\theta} \leq \theta u) = F_{\hat{\theta}}(\theta u) = (F_{x_1}(\theta u))^n
\end{equation*}
Como $X_i \sim U(0, \theta)$
\begin{gather*}
    F_{\hat{\theta}}(u) = (F_{x_1}(u))^n = 
    \begin{cases}
        0 \ si \ u\leq 0
        \\
        \left(\frac{u}{\theta}\right)^n \ si \ 0 < u < \theta
        \\
        1 \ si \ u\geq \theta
    \end{cases}
    \\
    =
    \begin{cases}
        0 \ si \ u\leq 0
        \\
        \left(\frac{u}{\theta}\right)^n \ si \ 0 < u < \theta
        \\
        1 \ si \ u\geq \theta
    \end{cases}
\end{gather*}

Por lo tanto, la distribucion de $\hat{\theta}/\theta$ no depende de $\theta$. Derivando, se obtiene la densidad de $\hat{\theta}/\theta$
\begin{equation*}
    f_{\hat{\theta}/ \theta} (u) = nu^{n-1}I_{(0,1)}(u)
\end{equation*}
Pivot:
\begin{equation*}
    H(X_1,X_2,\dots,X_n, \theta) = \frac{max(X_1,\dots,X_n)}{\theta}
\end{equation*}
Buscamos $a$ y $b$ tales que
\begin{equation*}
    P\left(a \leq \frac{max(X_1,\dots,X_n)}{\theta} \leq b \right) = 1 - \alpha
\end{equation*}
y, obtenemos el siguiente intervalo
\begin{equation*}
    \left[\frac{max(X_1,\dots,X_n)}{b},\frac{max(X_1,\dots,X_n)}{a} \right]
\end{equation*}
¿Y como elegimos $a$ y $b$? Debemos hallar $a$ y $b, 0 < a < b < 1$, tales que
\begin{equation}
    \int_{a}^b nw^{n-1}dw = w^n|_{a}^b = b^n-a^n = 1-\alpha
\end{equation}

Conviene elegir la solucion que produce el intervalo de menor longitud esperada, es decir, buscar $a$ y $b$ que minimicen $E(L)$ siendo
\begin{equation*}
    L = max(X_1,\dots,X_n)\left(\frac{1}{a} - \frac{1}{b}\right)
\end{equation*}
sujeto a la condicion $b^n - a^n = 1 - \alpha$
Como ya hemos demostrado que $E(max(X_1,\dots,X_n)) = \frac{n}{n+1} \theta$, debemos minimizar
\begin{equation*}
    \frac{n}{n+1}\theta\left(\frac{1}{a} - \frac{1}{b}\right)
\end{equation*}
sujeto a la condicion $b^n - a^n = 1-\alpha$
El intervalo de minima longitud esperada es
\begin{equation*}
    \left(\frac{max{X_1,\dots,X_n}}{1}, \frac{max(X_1,\dots,X_n)}{\sqrt[n]{\alpha}}\right)
\end{equation*}

\subsection{Intervalo de confianza asintotico para el parametro $p$ de la distribucion binomial}
\subsubsection{Aproximacion normal a la binomial}
Sea $X \sim Bi(n,p)$, entonces $X$ es el nuumero de exitos en n repeticiones de un experimento binomial con probabilidad de exito igual a p
Sea, para $i = 1,\dots,n$,
\begin{equation*}
    X_i = 
    \begin{cases}
        1 & \text{si se obtuvo exito en la $i$-esima repeticiones} 
        \\
        0 & \text{si se obtuvo fracaso en la $i$-esima repeticion} 
    \end{cases}
\end{equation*}

Estas v.a. son indepentientes, $X_i \sim Bi(1,p) \forall i$ y
\begin{equation*}
    X = \sum_{i=1}^n X_i
\end{equation*}
A demas $E(X_i) = p$ y $V(X_i) = p(1 - p)$. Entonces, por TCL
\begin{equation*}
    \frac{X - np}{\sqrt{np(1-p)}} \overset{a}{\sim} N(0,1) \ \text {y} \ \frac{\bar{X} - p}{\sqrt{p(1-p)/n}} \overset{a}{\sim} N(0,1)
\end{equation*}

Sea $X_1, X_2,\dots,X_n$ una m.a. de una distribucion B(1,p)
\begin{equation*}
    \frac{\bar{X} - p}{\sqrt{p(1-p)/n}} \overset{a}{\sim} N(0,1)
\end{equation*}
Implica que
\begin{equation*}
    P\left(-z_{\alpha/2} \leq \frac{\bar{X} - p}{\sqrt{\frac{p(1-p)}{n}}} \leq z_{\alpha/2}\right) \overset{n\rightarrow \infty}{\rightarrow} 1 - \alpha    
\end{equation*}

¿Como despejamos $p$? Mejor dicho, ¿despejamos $p$?
Recordar que en este contexto $\bar{X}$ tambien se nota $\widehat{p}$

Como, por la Ley de los Grandes Numeros
\begin{equation*}
    \widehat{p} = \bar{X} = \frac{\sum_{i=1}^n X_i}{n} \overset{p}{\rightarrow} p
\end{equation*}
podemos aplicar el teorema de Slutzky y reemplazar en el denominador el pivote $p$ por su estimador. Entonces se tiene que
\begin{equation*}
    \frac{\bar{X} - p}{\sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}} \overset{d}{\rightarrow} N(0,1)
\end{equation*}
y por lo tanto
\begin{equation*}
    P\left(-z_{\alpha/2} \leq \frac{\bar{X} - p}{\sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}} \leq z_{\alpha/2}\right) \cong 1 - \alpha    
\end{equation*}
Por lo tanto, un intervalo para $p$ de nivel asintotico $1-\alpha$ esperanza
\begin{equation*}
    \left[\bar{X} - z_{\alpha/2}\sqrt{\frac{\bar{X}(1-\bar{X})}{n}}, \bar{X} + z_{\alpha/2}\sqrt{\frac{\bar{X}(1-\bar{X})}{n}} \right]
\end{equation*}
o, lo que es lo mismo
\begin{equation*}
    \left[\widehat{p} - z_{\alpha/2}\sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}, \widehat{p} + z_{\alpha/2}\sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}} \right]
\end{equation*}

\end{document}
