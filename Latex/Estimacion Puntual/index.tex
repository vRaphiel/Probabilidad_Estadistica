\documentclass[a4paper]{article}
\usepackage{amsmath}
\setlength{\parskip}{1em}
\input{Algo1Macros}

\begin{document}

\section{Ejemplos de estimacion puntual}

\subsection{Ejercicio 1 - Distribucion Normal}
Se quiere conocer el peso medio de los paquetes de arroz producido por una fábrica. Para ello se toman 30 cajas
de arroz al azar y se las pesa. Se obtiene

\begin{equation*}
    \begin{matrix}
        0.96 & 0.97 & 1.12 & 1.16 & 1.03 & 0.95 & 0.91 & 0.87 & 0.96 & 1.04 \\
        0.77 & 0.99 & 0.84 & 1.08 & 1.12 & 0.78 & 0.95 & 0.93 & 1.09 & 0.92 \\
        1.00 & 0.92 & 1.02 & 0.90 & 0.87 & 0.85 & 1.03 & 1.04 & 0.92 & 1.07
    \end{matrix}
\end{equation*}

Supongamos que el peso de un paquete elegido al azar es una variable aleatoria $X \sim \mathcal{N}(\mu,\sigma^2)$

Al elegir $n$ paquetes tenemos: Sea $X_{1},X_{2},\dots,X_{n}$ i.i.d. $X_{i} \sim \mathcal{N}(\mu,\,\sigma^{2})$
Estimador de los momentos de $\mu$ de orden 1:
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\mu}}(X) = \widehat{\mu}
\end{equation*}
Con estos datos la estimacion que se obtiene es $\widehat{\mu}_{obs} = 0.97$

\subsection{Ejercicio 2 - Distribucion Exponencial}
Una fabrica de lamparas sabe que el tiempo de vida, en dias, de las lamparas que fabrica, sigue una distribucion $Exp(\theta)$.
Obtener una formula para estimar $\theta$ a partir de una muestra aleatoria $X_{1}\dots X_{n}$

Antes de probar las lamparas no sabemos cuanto durara cada una. Asi la duracion de la primera puede ser considerada
una v.a $X_{1}$. la segunda una v.a $X_{2}$, etc. 

\begin{equation*}
    X_{1}, X_{2}, \dots, X_{n} \sim Exp(\theta)
\end{equation*}

Para hallar el estimador de momentos de $\theta$, hay que despejar $\widehat{\theta}$ de
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\theta}}(X_{1})
\end{equation*}
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = \frac{1}{\mathcal{\theta}} \Rightarrow \mathcal{\theta} = \frac{1}{\widehat{X}}
\end{equation*}

\subsection{Ejercicio 3 - Distribucion uniforme}
\begin{equation*}
    X_{1}, X_{2}, \dots , X_{n} \sim \mathcal{U}{0, \theta}
\end{equation*}
Hay que despejar $\theta$ de
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\theta}}(X_{1})
\end{equation*}
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = \frac{\widehat{\theta}}{2} \Rightarrow \mathcal{\theta} = 2\frac{1}{n}\sum_{i=1}^{n} X_{i}
\end{equation*}

\subsection{Ejercicio 4 - Estimacion de ambos parametros de la normal}
Supongamos que tenemos una muestra aleatoria $X_{1}, X_{2},\dots,X_{n} \sim \mathcal{N}(\mu, \sigma^2)$ \\
Se tiene que $E_{\mu, \sigma^2}(X) = \mu$ y $E_{\mu, \sigma^2}(X^2) = \mu^2 + \sigma^2$
Para encontrar el estimador de momentos de $\mu$ y $\sigma$ hay que resolver el sistema

\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i} = E_{\widehat{\mu}, \widehat{\sigma}^2}(X_{1}) = \widehat{\mu}
\end{equation*}
\begin{equation*}
    \frac{1}{n} \sum_{i=1}^n X_{i}^2 = E_{\widehat{\mu}, \widehat{\sigma}^2}(X_{1}^2) = \widehat{\mu}^2 + \widehat{\sigma}^2
\end{equation*}

\begin{equation*}
    \begin{cases}
        \frac{1}{n} \sum_{i=1}^n X_{i} = \widehat{\mu}
        \\
        \frac{1}{n} \sum_{i=1}^n X_{i}^2 = \widehat{\mu}^2 + \widehat{\sigma}^2
        \end{cases}
\end{equation*}
\begin{equation*}
    \widehat{\mu} = \frac{1}{n} \sum_{i=1}^n X_{i}
\end{equation*}
\begin{equation*}
    \widehat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_{i}^2 - (\frac{1}{n} \sum_{i=1}^n X_{i})^2
\end{equation*}

\section{Ejemplos de estimacion puntual - Verosimilitud}
\subsection{Ejercicio 1 - $\mathcal{E}(\lambda):f(x,\lambda) = \lambda e^{-x\lambda}\mathcal{I}_{(0, \infty)}(x)$}
$X_{1},\dots,X_{n}$ v.a. i.i.d. $X_{i} \sim \mathcal{E}(\lambda), \lambda > 0$
\begin{equation*}
    L(\lambda; x) = \prod_{i=1}^n f(x_{i}, \lambda) = \prod_{i=1}^n \lambda e^{-x_{i}\lambda}\mathcal{I}_{(0,\infty)}(x_{i})
\end{equation*}
Si $x_{i}\geq 0 \forall i$
\begin{equation*}
    L(\lambda; x) = \lambda e^{-x_{i}\sum_{i=1}^n x_{i}}
\end{equation*}
Si consideramos $\log L$ resulta
\begin{equation*}
    l(\lambda;x) = n \log(\lambda)-\lambda \sum_{i=1}^n x_{i}
\end{equation*}
Derivando e igualando a 0 queda \\
$\frac{n}{\lambda} - \sum_{i=1}^n x_{i} = 0 \Rightarrow$ punto critico es $\frac{1}{\bar{x}_{n}}$, ver que maximiza \\
$\Rightarrow \widehat{\lambda} = \frac{1}{\bar{X}_{n}}$

\subsection{Ejercicio 2 - $X_{1},\dots,X_{n}$ v.a. i.i.d. $X_{i} \sim \mathcal{N} (\mu, 9)$, $f(x,\mu, 9) = \frac{1}{\sqrt{2\pi}}\frac{1}{3}e^{-\frac{1}{2}\frac{(x-\mu)^2}{9}}$ }
\begin{equation*}
    L(\mu, 9; x) = \prod_{i=1}^n f(x_{i}, \mu, 9) = \prod_{i=1}^n \frac{1}{\sqrt(2\pi)}\frac{1}{3}e^{-\frac{1}{2}\frac{(x_{i}-\mu)^2}{9}}
\end{equation*}
\begin{equation*}
    L(\mu, 9; x) = (\frac{1}{\sqrt(2\pi)})^n(\frac{1}{3})^n e^{-\frac{1}{2}\frac{\sum_{i=1}^n(x_{i}-\mu)^2}{9}}
\end{equation*}
Tomemos logatirmo
\begin{equation*}
    l(\mu, 9; x) = cte - \frac{1}{2}\frac{\sum_{i=1}^n(x_{i}-\mu)^2}{9}
\end{equation*}
Maximizar a $l(\mu, 9; x)$ como funcion de $\mu$ equivale a minimizar
\begin{equation*}
    h(\mu) = \sum_{i=1}^n (x_{i} - \mu)^2
\end{equation*}
Un par de clases aatras vimos que $h(\mu)$ se minimiza en $\bar{x}_{n}$
\begin{equation*}
    EMV de \ \mu : \widehat{\mu} = \bar{X}_{n}
\end{equation*}
\begin{equation*}
    L(\mu, 9; x) = \prod_{i=1}^n f(x_{i}, \mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt(2\pi\sigma^2)}e^{-\frac{(x_{i}-\mu)^2}{2\sigma^2}}
\end{equation*}
\begin{equation*}
    L(\mu, 9; x) = \prod_{i=1}^n f(x_{i}, \mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt(2\pi\sigma^2)}e^{-\sum_{i=1}^n \frac{(x_{i}-\mu)^2}{2\sigma^2}}
\end{equation*}
Tomando logatirmo y resolviendo las ecuaciones
\begin{equation*}
    \frac{\partial l (\mu, \sigma^2;x)}{\partial \mu} = 0 \ y \ \frac{\partial l (\mu, \sigma^2;x)}{\partial \sigma^2} = 0
\end{equation*}
se obtiene que los EMV de $\mu$ y $\sigma^2$ section
\begin{equation*}
    \widehat{\mu} = \bar{X}_{n} \ \ \ \ \widehat{\sigma}^2 = \frac{\sum_{i=1}^n ((X_{i} - \bar{X}_n))^2}{n}
\end{equation*}
Los estimadores de $\mu$ y $\sigma$ coinciden con los estimadores de momentos

\subsection{Ejercicio 2 - $\mathcal{U}(0,\theta):f(x,\theta) = \frac{1}{\theta}\mathcal{I}_{(0,\theta)}(x)$}
$X_{1},\dots,X_{n}$ v.a. i.i.d. $X_{i} \sim \mathcal{U}(0,\theta)$
\begin{equation*}
    f(x_{1},\dots,x_{n}, \theta) = \prod_{i=1}^n \frac{1}{\theta}I_{(0,\theta)}(x_{i}) = \frac{1}{\theta^n}\prod_{i=1}^n I_{(0,\theta)}(x_{i})
\end{equation*}
La indicadora vale 1 cuando todas las indicadoras valgan 1, de lo contrario, es 0
Por ende, la conjunta puede tomar 2 valores:
\begin{equation*}
    \begin{cases}
        \frac{1}{\theta^n} \ si \ 0<x_{i}<\theta \  \forall i
        \\
        0 \ en \ otro \ caso
        \end{cases}
\end{equation*}
\begin{equation*}
    \begin{cases}
        \frac{1}{\theta^n} \ si \ \theta > \max(x_{i})
        \\
        0 \ en \ otro \ caso
        \end{cases}
\end{equation*}

\pagebreak

\section{Propiedades de estimacion}
\subsection{Ejercicio 1 - Distribucion Uniforme}
\begin{itemize}
    \item $X_i \sim \mathcal{U}(0, \theta)$
    \item $\widehat{\theta} = 2\bar{X}_n$ (Estimador de momentos)
    \item $\widehat{\theta} = max{X_{1},\dots,X_{n}}$ (Estimador de maxima Verosimilitud)
    \item Calcule la esperanza y varianza de cada estimador
\end{itemize}

Esperanza del estimador de momentos
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$. El estimador de momentos de $\theta$ es $\widehat{\theta} = 2\bar{X}$

\begin{equation*}
    E_{\theta}(\widehat{\theta}) = 2E_{\theta}\bar{X} = 2\frac{\theta}{2} = \theta \ \ \forall \theta
\end{equation*}

Recordar

Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$
\begin{itemize}
    \item El EMV de $\theta = max_{1\leq i \leq n}(X_{i})$
    \item La fda es $\widehat{\theta}$ es
        \begin{equation*}
            F_{\widehat{\theta}}(u) = (F_{X_{i}}(u))^n =       
            \begin{cases}
                0 \ si \ u \leq \theta
                \\
                (\frac{u}{\theta})^n \ si \ 0<u<\theta
                \\
                1 \ si \ u \geq \theta
            \end{cases}
        \end{equation*}
    \item La densidad de $\widehat{\theta}$ es
        \begin{equation*}
            f_{\widehat{\theta}}(u) = n\frac{u}{\theta}^{n-1} \frac{1}{\theta} I_{(0,\theta)}(u)
        \end{equation*}
\end{itemize}

\begin{equation*}
    E_{\theta}(\widehat{\theta}) = \int_{0}^{\theta} un(\frac{u}{\theta})^{n-1} \frac{1}{\theta} du = 
    \\
    \frac{n}{\theta^n}\int_{0}^{\theta} u^n du = \frac{n}{\theta^n}\frac{u^{n+1}}{n+1}\Biggr|_{0}^{\theta} = \frac{n}{n+1}\theta
\end{equation*}

Varianza del estimador de momentos

Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$ y $\widehat{\theta}=2\bar{X}$ el estimador de momentos de $\theta$
\begin{equation*}
    V_{\theta}(\widehat{\theta}) = 4V_{\theta}(\bar{X}) = 4 \frac{\theta^2/12}{n}
\end{equation*}

Varianza del estimador de maxima Verosimilitud
Recordar la densidad del EMV
\begin{gather*}
    f_{\theta}(u) = n\frac{u}{\theta}^{n-1}\frac{1}{\theta}I_{(0,\theta)}(u)
    \\
    E_{\theta}(\widehat{\theta}^2) = \int_{0}^{\theta} u^2n \frac{u}{\theta}^{n-1} \frac{1}{\theta}du = \frac{n}{\theta^n}\int_{0}^{\theta}u^{n+1} du
    \\
    \frac{n}{\theta^n}\frac{u^{n+2}}{n+2}\Biggr|_{0}^{\theta} = \frac{n}{n+2}\theta^2
\end{gather*}
Entonces,
\begin{gather*}
    V_{\theta}(\widehat{\theta}) = \frac{n}{n+2}\theta^2 - (\frac{n}{n+1})^2\theta^2=(\frac{n}{n+2} - \frac{n^2}{(n+1)^2})\theta^2
    \\
    \frac{n}{(n+2)(n+1)^2}\theta^2
\end{gather*}

\subsection{Ejercicio 2 - Sesgo de los EMV de $\mu$ y $\sigma^2$ en el caso normal}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{N}(\mu, \sigma^2)$. Los EMV de $\mu$ y $\sigma^2$ section
\begin{equation*}
    \widehat{\mu} = \bar{X} \ \ \ \ \widehat{\sigma}^2 = \frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{n}
\end{equation*}

\begin{itemize}
    \item El estimador de $\mu$ es insesgado pues $E_{\mu, \sigma^2}(\widehat{\mu}) = \mu \forall\mu$,
    \item El estimador de $\sigma^2$ es sesgado pero es asintoticamente insesgado pues:
\end{itemize}
\begin{gather*}
    E_{\mu, \sigma^2}(\widehat{\sigma}^2) = E_{\mu, \sigma^2}(\frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{n})
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n (X_{i}^2 - 2X_{i}\bar{X} + \bar{X}^2))
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2 - 2\bar{X}\sum_{i=1}^n X_{i} + n\bar{X}^2)
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2 - 2n\bar{X}^2 + n\bar{X}^2)
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2 - n\bar{X}^2)
    \\
    = \frac{1}{n}E_{\mu, \sigma^2}(\sum_{i=1}^n X_{i}^2) - E_{\mu, \sigma^2}(\bar{X}^2)
    \\
    = \frac{n}{n}E_{\mu, \sigma^2}(X_{1}^2) - E_{\mu, \sigma^2}(\bar{X}^2)
    \\
    = [ V_{\mu, \sigma^2}(X_{1}) + (E_{\mu, \sigma^2}(X_{1}))^2] - [ V_{\mu, \sigma^2}(\bar{X}) + (E_{\mu, \sigma^2}(\bar{X_{1}}))^2]
    \\
    = \sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2
    \\
    = \frac{n-1}{n}\sigma^2
\end{gather*}

\subsection{Ejemplo - Consistencia de la media muestral}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion con $E(X_{i})=\mu$ y $V(X_{i}) = \sigma^2 < \infty$ 
Sea $\widehat{\mu} = \bar{X}$
\begin{itemize}
    \item $E(\bar{X}) = \mu$
    \item $V(\bar{X}) = \frac{\sigma^2}{n}$
\end{itemize}
Entonces $\widehat{\mu}$ es un estimador consistente de $\mu$

\subsection{Ejemplo - Consistencia de los estimadores de $\theta$ en la $\mathcal{U}[0,\theta]$}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una distribucion $\mathcal{U}(0, \theta)$. Vimos que el EMV de $\theta, \widehat{\theta} = max_{1\leq i \leq n}(X_{i})$ y 
\begin{itemize}
    \item $E_{\theta}(\widehat{\theta}) = \frac{n}{n+1}\theta \Rightarrow \widehat{\theta}$ es asintóticamente insesgado
    \item $V_{\theta}(\widehat{\theta}) = \frac{n}{(n+2)(n+1)^2}\theta^2 \rightarrow_{n\rightarrow\infty}0$
\end{itemize}
Entonces el $\widehat{\theta}$ es consistente

\subsection{Ejemplo - Consistencia de la varianza muestral}
Sea $X_{1},X_{2},\dots,X_{n}$ una m.a de una con $E(X_{i}) = \mu$ y $V(X_{i}) = \sigma^2 < \infty$ entonces la varianza muestral $S^2$ es un estimador
consistente de la varianza poblacional
\begin{gather*}
    S^2 = \frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{n-1} = \frac{1}{n-1}(\sum_{i=1}^n X_{i}^2 - n\bar{X}^2)
    \\
    = \frac{n}{n-1}(\frac{\sum_{i=1}^n X_{i}^2}{n} - \bar{X}^2)
\end{gather*}

Por la Ley de los Grandes Numeros $\bar{X} \xrightarrow[]{p} \mu$, entonces por la propiedad 4,
\begin{equation*}
    \bar{X}^2 \xrightarrow[p]{} \mu^2
\end{equation*}

Por otra parte, aplicando nuevamente la Ley de los grandes Numeros
\begin{equation*}
    \frac{\sum_{i=1}^n X_{i}^2}{n} \xrightarrow[]{p} E_{\mu, \sigma^2}(X^2) = V_{\mu, \sigma^2}(X) + [E_{\mu, \sigma^2}(X)]^2 = \sigma^2 + \mu^2
\end{equation*}
Como ademas $\frac{n}{n-1} \rightarrow 1$, se obtiene
\begin{equation*}
    S_{x}^2 = \frac{n}{n-1}(\frac{\sum_{i=1}^n X_{i}^2}{n} - \bar{X}^2) \xrightarrow[]{p} \sigma^2 + \mu^2 -\mu^2 = \sigma^2
\end{equation*}
y por lo tanto la varianza muestral es un estimador consistente de $\sigma^2$

\end{document}
